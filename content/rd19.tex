\section {GLM one parameter}
%%% RD19

% Vector variables (in Bold Font style)
\newcommand{\va}{\mathbf{a}} \newcommand{\vb}{\mathbf{b}} \newcommand{\vc}{\mathbf{c}} \newcommand{\vd}{\mathbf{d}} \newcommand{\ve}{\mathbf{e}} \newcommand{\vf}{\mathbf{f}} \newcommand{\vg}{\mathbf{g}} \newcommand{\vh}{\mathbf{h}} \newcommand{\vi}{\mathbf{i}} \newcommand
{\vj}{\mathbf{j}} \newcommand{\vk}{\mathbf{k}} \newcommand{\vl}{\mathbf{l}} \newcommand{\vm}{\mathbf{m}} \newcommand{\vn}{\mathbf{n}} \newcommand{\vo}{\mathbf{o}} \newcommand{\vp}{\mathbf{p}} \newcommand{\vq}{\mathbf{q}} \newcommand{\vr}{\mathbf{r}} \newcommand{\vs}{\math
bf{s}} \newcommand{\vt}{\mathbf{t}} \newcommand{\vu}{\mathbf{u}} \newcommand{\vv}{\mathbf{v}} \newcommand{\vw}{\mathbf{w}} \newcommand{\vx}{\mathbf{x}} \newcommand{\vy}{\mathbf{y}} \newcommand{\vz}{\mathbf{z}} 
\newcommand{\vA}{\mathbf{A}} \newcommand{\vB}{\mathbf{B}} \newcommand{\vC}{\mathbf{C}} \newcommand{\vD}{\mathbf{D}} \newcommand{\vE}{\mathbf{E}} \newcommand{\vF}{\mathbf{F}} \newcommand{\vG}{\mathbf{G}} \newcommand{\vH}{\mathbf{H}} \newcommand{\vI}{\mathbf{I}} \newcommand{\vJ}{\mathbf{J}} \newcommand{\vK}{\mathbf{K}} \newcommand{\vL}{\mathbf{L}} \newcommand{\vM}{\mathbf{M}} \newcommand{\vN}{\mathbf{N}} \newcommand{\vO}{\mathbf{O}} \newcommand{\vP}{\mathbf{P}} \newcommand{\vQ}{\mathbf{Q}} \newcommand{\vR}{\mathbf{R}} \newcommand{\vS}{\mathbf{S}} \newcommand{\vT}{\mathbf{T}} \newcommand{\vU}{\mathbf{U}} \newcommand{\vV}{\mathbf{V}} \newcommand{\vW}{\mathbf{W}} \newcommand{\vX}{\mathbf{X}} \newcommand{\vY}{\mathbf{Y}} \newcommand{\vZ}{\mathbf{Z}} 

% Greek Vector variables (in Bold Font style)
\newcommand{\vbeta}{\bm{\beta}} 
\newcommand{\vbhat}{\bm{\hat \beta}}
\newcommand{\vbstar}{\bm{\beta^*}}
\newcommand{\veps}{\bm{\epsilon}}
\newcommand{\vmu}{\bm{\mu}}
\newcommand{\vtheta}{\bm{\theta}}
\newcommand{\valpha}{\bm{\alpha}}
\newcommand{\vdelta}{\bm{\delta}}

% Constant vectors
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vone}{\mathbf{1}}
\newcommand{\thetac}{\mathrm{\theta^{(0)}}}
\newcommand{\norm}[1]{\|#1\|_2} % l2 norm for ordinary vectors

% Scalars

% Matrix variables (in DS style where possible)
\newcommand{\mA}{\mathds{A}} \newcommand{\mB}{\mathds{B}} \newcommand{\mC}{\mathds{C}} \newcommand{\mD}{\mathds{D}} \newcommand{\mE}{\mathds{E}} \newcommand{\mF}{\mathds{F}} \newcommand{\mG}{\mathds{G}} \newcommand{\mH}{\mathds{H}} \newcommand{\mI}{\mathds{I}} \newcommand{\mJ}{\mathds{J}} \newcommand{\mK}{\mathds{K}} \newcommand{\mL}{\mathds{L}} \newcommand{\mM}{\mathds{M}} \newcommand{\mN}{\mathds{N}} \newcommand{\mO}{\mathds{O}} \newcommand{\mP}{\mathds{P}} \newcommand{\mQ}{\mathds{Q}} \newcommand{\mR}{\mathds{R}} \newcommand{\mS}{\mathds{S}} \newcommand{\mT}{\mathds{T}} \newcommand{\mU}{\mathds{U}} \newcommand{\mV}{\mathds{V}} \newcommand{\mW}{\mathds{W}} \newcommand{\mX}{\mathds{X}} \newcommand{\mY}{\mathds{Y}} \newcommand{\mZ}{\mathds{Z}}

% Greek matrix variables
\newcommand{\msigma}{\Sigma}
\newcommand{\mbeta}{\mathds{B}}

\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}

\newcommand{\deq}{:=}
\newcommand{\conv}[2]{{#1 * #2}} % convolution of two functions
\newcommand{\abs}[1]{|{#1}|}
\newcommand{\ind}[1]{\mathds{1}(#1)} % Indicator function
\newcommand{\PP}{\mathrm{P}} % Probability
\newcommand{\EE}{\mathrm{E}} % Expectation



\begin {equation} \begin {split} 
& f_\theta(y) = \exp (\frac {y \theta - b(\theta)} {\phi} + c(y, \phi)) \\
& b(), c() \text { are known. $\phi$ is called the dispersion parameter} \\
\end {split} \end {equation}

\begin {equation} \begin {split} 
& f_\theta(y) = \exp (\frac {y \theta - b(\theta)} {\phi} + c(y, \phi)) \\
& b(), c() \text { are known. $\phi$ is called the dispersion parameter} \\
\end {split} \end {equation}

A canonical link function forces:
\begin {equation} \begin {split}
& g(\mu) = \theta \\
& \therefore \mu = g^{-1}(\theta) \\
& \implies g^{-1}(\theta) =  b^{'}(\theta) \\
& \implies g = b{'}^{-1} \\
\end {split} \end {equation}

The log-likelihood is:
\begin {equation} \begin {split} 
l_n(\vY, \mX, \vbeta) = \frac { \sum \limits_{i=1}^{n} Y_i \vX_i^T \vbeta -b(\vX_i^T \vbeta) } {\phi}  + \sum \limits_{i=1}^{n} c(Y_i, \phi) \\
\end {split} \end {equation}

$\vbeta$ can be estimated as:
\begin {equation} \begin {split}
& \hat \vbeta = \argmax_{\vbeta} (l_n(\vY, \mX, \vbeta)) \\
\end {split} \end {equation}

 Distribution | $b(\theta)$ | $g(\mu)$ \\
 Gaussian | $ \frac {\theta^2} {2}$ | $ \mu $ \\
 Poisson | $ e^{\theta} $ | $ \ln(\mu) $ \\
 Bernoulli | $ \ln (1 + e^{\theta}) $ | $ \ln (\frac {\mu} {1 - \mu}) $ \\
 Gamma | $ -\ln (-\theta) $ | $ \frac {-1} {\mu} $ \\

\section {Convolution between two functions} 
The convolution of two continuous functions is defined as:
\begin{equation} \begin {split}
& (\conv {f} {g}) (t) \deq \int_{- \infty}^{\infty} f(\tau) g(t - \tau) d\tau
\end {split} \end{equation} 

The discrete analog is:
\begin{equation} \begin {split}
& (\conv {f} {g}) (n) \deq \sum_{m = -\infty}^{\infty} f(m) g(n - m)
\end {split} \end{equation}

\section {Kolmogrov-Smirnov (KS) test}
\begin {equation} 
\begin {split}
& \text {$X_i$ are reordered so that } X_{(1)} \le X_{(2)} \le ... X_{(n)} \\
& T_n = \sqrt{n} \max \limits_{i=1...n} \{ \max (\lvert \frac {i-1}{n} - F^0(X_{(i)}) \rvert , \lvert \frac {i}{n} - F^0(X_{(i)}) \rvert ) \} \\
& \text {At level $\alpha$, } \psi_{\alpha}  = \mathds{1} (\frac {T_n}{\sqrt{n}} > q_{\alpha}) \\
& \text {Where $q_{\alpha}$ is the $1 - \alpha$ quantile obtained from KS tables} \\
\end {split}
\end {equation}

\section {Kolmogrov-Lilliefors (KL) test}
\begin {equation} \begin {split}
& T_n = \sqrt{n} \underset{i = 1 \ldots n} {\max} \abs {F_n(X_i) - \Phi_{\hat \mu, \hat \sigma^2}(X_i)} \\
& \text {At level $\alpha$, } \psi_{\alpha}  = \ind{\frac {T_n}{\sqrt{n}} > q_{\alpha}} \\
& \text {Where $q_{\alpha}$ is the $1 - \alpha$ quantile from KL tables} \\
\\
\\
\end {split} \end {equation}

\section {Tips}
\begin {enumerate}
\item (Continuous convolution) The area under the curve for the convolution expression is the product of the areas under the individual components of the convolution. See ML L12, tab 2. (This formulation regarding the area of the convolution likely has caveats).
\item The expected value of a non-negative continuous random variable $X$ = $\EE[X] = \int \limits_0^\infty x f_X(x) dx = \int \limits_0^\infty \PP (X > t) dt$
\item $\mX^T \mX = \sum \limits_{i=1}^{n} \vX_i \vX_i^T$ where $\mX \equiv n \times d + 1, \vX_i \equiv 1 \times d + 1$
\item $\mX^T \vY = \sum \limits_{i=1}^{n} Y_i \vX_i$
\end {enumerate}

\section {Sums}
\begin {equation} \begin {split}
& \frac {1} {n} {\sum \limits_{i=1}^{n} (X_i - \mu)^2} = \frac {1} {n} {\sum \limits_{i=1}^{n} (X_i - \overline{X_n})^2} = \overline{(X_n^2)} - (\overline{X_n})^2  \\
& \text {Where } \overline{(X_n^2)} = \frac {1}{n} (\sum \limits_{i=1}^{n} X_i^2) \text { and } \overline{X_n} = \frac {1}{n} (\sum \limits_{i=1}^{n} X_i) \\
\end {split} \end {equation}

% From Ben Lambert on Youtube
\begin {equation} \begin {split}
\sum \limits_{i=1}^{n}(X_i - \bar X_n)(Y_i - \bar Y_n) = \sum \limits_{i=1}^{n}(X_i - \bar X_n)Y_i = \sum \limits_{i=1}^{n}X_i(Y_i - \bar Y_n)
\end {split} \end {equation}

\section {Differentiating the dot product of two vectors}
\begin {equation} \begin {split}
& q = \vX^T \vbeta = \dot {\vbeta} {\vX} , \text {where $\vX, \vbeta$ are both $d \times 1$ vectors} \\
& \frac {dq} {d \vbeta} = \vX \\
& \frac {dq} {d (\vbeta^T)} = \vX^T \\
\end {split} \end {equation}

Differentiating the norm of a vector:
\begin {equation} \begin {split}
& q = \norm{\vbeta}^2 =  \text {where $\vbeta$ is a $d \times 1$ vector} \\
& \frac {dq} {d \vbeta} = 2 \vbeta \\
\end {split} \end {equation}

Differentiating the squared error:
\begin {equation} \begin {split}
& q = (Y - \dot {\vbeta^T} {\vX})^2, \text {where $\vX, \vbeta$ are both $d \times 1$ vectors} \\
& \frac {dq} {d \vbeta} = 2 (Y - \dot {\vbeta^T} {\vX}) (- \vX) \\
\end {split} \end {equation}

Differentiating the quadratic form:
\begin {equation} \begin {split}
& q = \vX^T \mA \vX, \text {where $\vX$ is a $d \times 1$ vector and $\mA$ is a $d \times d$ symmetric matrix} \\
& \frac {dq} {d \vX} = 2 \mA \vX \\
& \frac {dq} {d \vX^T} = (2 \mA \vX)^T = 2 \vX^T \mA^T = 2 \vX^T \mA \\
\end {split} \end {equation}

Differentiating a power of $e$:
\begin {equation} \begin {split}
& q = e^{\vX^T \vbeta} \text {where $\vX, \vbeta$ are both $d \times 1$ vectors} \\
& \frac {dq} {d \vbeta} = \vX e^{\vX^T \vbeta} \\
\end {split} \end {equation}

